{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 17:24:00.134351: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 17:24:00.789078: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-28 17:24:00.789128: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-28 17:24:00.863987: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-28 17:24:02.024953: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-28 17:24:02.025635: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-28 17:24:02.025644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f207c909000>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[252], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = (X_train / 255) -0.5\n",
    "X_test = (X_test / 255) -0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 12:00:01.965855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-16 12:00:01.965966: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-16 12:00:01.966014: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-16 12:00:01.966058: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-16 12:00:01.966144: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-16 12:00:01.966227: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-16 12:00:01.966273: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-16 12:00:01.966333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-11-16 12:00:01.966376: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-16 12:00:01.966383: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-16 12:00:01.967050: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "X_train = expand_dims(X_train, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "üëâ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4,4), input_shape=(28, 28, 1), padding='same', activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, (3,3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2))) \n",
    "    \n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu')) \n",
    "    \n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    ### Model compilation\n",
    "    model.compile(optimizer = \"adam\",\n",
    "                  loss = \"categorical_crossentropy\",\n",
    "                  metrics=\"accuracy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1* 4*4 * 8 + 8\n",
    "8*(3*3 * 16)+ 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 12, 12, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5770      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,184\n",
      "Trainable params: 7,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f207c44b280>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping()\n",
    "\n",
    "model.fit(X_train, y_train_cat,\n",
    "          batch_size=16,\n",
    "          epochs=5,\n",
    "          validation_split = 0.3,\n",
    "          callbacks=[es],\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0994 - accuracy: 0.9704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09940739721059799, 0.9703999757766724]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
